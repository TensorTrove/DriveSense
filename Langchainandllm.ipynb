{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai\n",
        "!pip install langchain\n",
        "!pip install langchain_community # Install the missing module\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "fiBaoS8lde-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain-google-genai import GoogleGenerativeAI"
      ],
      "metadata": {
        "id": "z7vNiDyhddFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QepD6157dSsP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'API_KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GooglePalm(model_name=\"models/text-bison-001\", temperature=0.7)\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"\n",
        "You are an expert chatbot specialized in providing accurate information about Machu Picchu, the ancient Incan city in Peru.\n",
        "Focus on giving factual and interesting information about Machu Picchu's history, architecture, and cultural significance.\n",
        "Always provide clear and concise answers directly related to the question asked.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "# Initialize the conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI\"),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def run_machu_picchu_bot(input_text: str) -> str:\n",
        "    try:\n",
        "        response = conversation.invoke(input=input_text)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"I apologize, but I encountered an error: {str(e)}. Could you please try asking your question again?\"\n",
        "\n",
        "# Example usage\n",
        "questions = [\n",
        "    \"Hello\",\n",
        "    \"where is it located\",\n",
        "    \"i have visited before so i want to know what is the currency called ?\",\n",
        "    \"name of the person whose face is on it\",\n",
        "    \"convert it to INR\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"Human: {question}\")\n",
        "    answer = run_machu_picchu_bot(question)\n",
        "    print(f\"AI: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'API-KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key='API-KEY')\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"\n",
        "You are an expert chatbot specialized in providing accurate information about Machu Picchu, the ancient Incan city in Peru.\n",
        "Focus on giving factual and interesting information about Machu Picchu's history, architecture, and cultural significance.\n",
        "Always provide clear and concise answers directly related to the question asked.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "# Initialize the conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI\"),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def run_machu_picchu_bot(input_text: str) -> str:\n",
        "    try:\n",
        "        response = conversation.invoke(input=input_text)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"I apologize, but I encountered an error: {str(e)}. Could you please try asking your question again?\"\n",
        "\n",
        "# Example usage\n",
        "questions = [\n",
        "    \"Hello\",\n",
        "    \"where is it located\",\n",
        "    \"i have visited before so i want to know what is the currency called ?\",\n",
        "    \"name of the person whose face is on it\",\n",
        "    \"convert it to INR\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"Human: {question}\")\n",
        "    answer = run_machu_picchu_bot(question)\n",
        "    print(f\"AI: {answer}\\n\")"
      ],
      "metadata": {
        "id": "q_bXGra5daFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
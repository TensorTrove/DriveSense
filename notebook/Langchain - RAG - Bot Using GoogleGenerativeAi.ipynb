{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain - RAG - Bot Using GoogleGenerativeAi\n",
        "\n",
        "This project involves creating a Car AI Assistant capable of performing various tasks based on user commands. The assistant leverages a Large Language Model for performing these tasks. This is a step by step guide showing how various things are implemented throughout to improve the Code part by part.\n",
        "\n",
        "[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)"
      ],
      "metadata": {
        "id": "pMnDPPHRURt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai\n",
        "!pip install -U langchain\n",
        "!pip install -U langchain_community\n",
        "!pip install -U huggingface_hub\n",
        "!pip install -U transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U google-generativeai\n",
        "!pip install -U fuzzywuzzy\n",
        "!pip install -U duckduckgo-search\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -U pypdf\n",
        "!pip install -U chromadb\n",
        "!pip install -U faiss-cpu\n",
        "!pip install -U pyngrok\n",
        "!pip install -U fastapi uvicorn"
      ],
      "metadata": {
        "id": "fiBaoS8lde-K",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Using Bert\n",
        "\n",
        "This method was the initial thinking but later was transformed into a more stable form using langchain and llm integration\n",
        "\n"
      ],
      "metadata": {
        "id": "azgZfOhNkWx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import requests\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Initialize the NLP model\n",
        "nlp = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"39f58c559276ac165d4c6facf36dab3e\"\n",
        "\n",
        "def process_command(command):\n",
        "    context = \"\"\"\n",
        "    Your car's AI assistant can perform various tasks:\n",
        "    1. Play music of different genres\n",
        "    2. Navigate to destinations using various routing options\n",
        "    3. Adjust climate settings in the car\n",
        "    4. Provide weather information for different cities\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        action = nlp(question=command, context=context)['answer']\n",
        "        print(f\"Interpreted action: {action}\")\n",
        "\n",
        "        if fuzz.partial_ratio(\"play music\", action.lower()) > 70:\n",
        "            response = f\"Playing {user_preferences['music']} music for you. Enjoy!\"\n",
        "        elif fuzz.partial_ratio(\"navigate\", action.lower()) > 70:\n",
        "            response = f\"Setting up navigation using your preferred {user_preferences['navigation']}. Where would you like to go?\"\n",
        "        elif fuzz.partial_ratio(\"climate\", action.lower()) > 70:\n",
        "            response = f\"Adjusting the climate to your preferred {user_preferences['climate']} setting. Let me know if you need any changes.\"\n",
        "        elif fuzz.partial_ratio(\"weather\", action.lower()) > 70:\n",
        "            response = \"I can provide weather information. Which city would you like to know about?\"\n",
        "        else:\n",
        "            response = \"I'm not sure I understood that command. Could you please rephrase it?\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        response = \"I encountered an error while processing your request. Could you try again?\"\n",
        "\n",
        "    return response\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"],\n",
        "            \"icon\": data[\"weather\"][0][\"icon\"]\n",
        "        }\n",
        "        return weather_data\n",
        "    else:\n",
        "        return {\"error\": \"City not found\"}\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your car's AI assistant! How can I help you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"Your command: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        response = process_command(user_input)\n",
        "        print(response)\n",
        "\n",
        "        if \"weather\" in response.lower():\n",
        "            city = input(\"Enter the city name: \").strip()\n",
        "            weather_info = get_weather(city)\n",
        "            if \"error\" in weather_info:\n",
        "                print(f\"I'm sorry, I couldn't find weather information for {city}.\")\n",
        "            else:\n",
        "                print(f\"Here's the weather in {city}:\")\n",
        "                print(f\"Temperature: {weather_info['temperature']}Â°C\")\n",
        "                print(f\"Description: {weather_info['description']}\")\n",
        "                print(f\"Icon code: {weather_info['icon']}\")"
      ],
      "metadata": {
        "id": "bCd3vvHFy2jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# First Langchain Integration\n",
        "\n",
        "The assistant can process commands related to music, navigation, climate control, and weather updates. By utilizing LangChain's advanced conversational capabilities and integrating with a weather API, the assistant offers a more intelligent and interactive user experience compared to the previous implementation.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gj-Kyt4DktKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "import requests\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR-API-KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"KEY\"\n",
        "\n",
        "# Define the prompt template for car assistant\n",
        "car_template = \"\"\"\n",
        "You are an AI assistant for a smart car. You can help with playing music, navigation, climate control, and providing weather information.\n",
        "Current user preferences:\n",
        "- Music: {music_preference}\n",
        "- Navigation: {navigation_preference}\n",
        "- Climate: {climate_preference}\n",
        "\n",
        "Focus on understanding and responding to car-related commands and queries.\n",
        "If multiple actions are requested, address them in the order mentioned.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "car_prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\", \"music_preference\", \"navigation_preference\", \"climate_preference\"],\n",
        "    template=car_template\n",
        ")\n",
        "\n",
        "# Initialize the memory\n",
        "memory = ConversationBufferMemory(input_key=\"input\", memory_key=\"history\")\n",
        "\n",
        "# Initialize the LLMChain for car assistant\n",
        "car_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=car_prompt,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def process_command(command):\n",
        "    try:\n",
        "        response = car_chain.predict(\n",
        "            input=command,\n",
        "            music_preference=user_preferences[\"music\"],\n",
        "            navigation_preference=user_preferences[\"navigation\"],\n",
        "            climate_preference=user_preferences[\"climate\"]\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"I encountered an error while processing your request. Could you try again?\"\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"],\n",
        "            \"icon\": data[\"weather\"][0][\"icon\"]\n",
        "        }\n",
        "        return weather_data\n",
        "    else:\n",
        "        return {\"error\": \"City not found\"}\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your car's AI assistant! How can I help you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"Your command: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        response = process_command(user_input)\n",
        "        print(\"AI:\", response)\n",
        "\n",
        "        if \"weather\" in response.lower() and \"which city\" in response.lower():\n",
        "            city = input(\"Enter the city name: \").strip()\n",
        "            weather_info = get_weather(city)\n",
        "            if \"error\" in weather_info:\n",
        "                print(f\"I'm sorry, I couldn't find weather information for {city}.\")\n",
        "            else:\n",
        "                print(f\"Here's the weather in {city}:\")\n",
        "                print(f\"Temperature: {weather_info['temperature']}Â°C\")\n",
        "                print(f\"Description: {weather_info['description']}\")\n",
        "                print(f\"Icon code: {weather_info['icon']}\")"
      ],
      "metadata": {
        "id": "P3FPMsNqN0pI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae96a42-d430-43b0-8915-657b39eb0155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to your car's AI assistant! How can I help you today?\n",
            "Your command: i feel cold\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are an AI assistant for a smart car. You can help with playing music, navigation, climate control, and providing weather information.\n",
            "Current user preferences:\n",
            "- Music: rock\n",
            "- Navigation: shortest route\n",
            "- Climate: cool\n",
            "\n",
            "Focus on understanding and responding to car-related commands and queries.\n",
            "If multiple actions are requested, address them in the order mentioned.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: i feel cold\n",
            "AI:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I can adjust the temperature for you. Would you like it to be warmer? \n",
            "\n",
            "Your command: on my way to home play some music\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are an AI assistant for a smart car. You can help with playing music, navigation, climate control, and providing weather information.\n",
            "Current user preferences:\n",
            "- Music: rock\n",
            "- Navigation: shortest route\n",
            "- Climate: cool\n",
            "\n",
            "Focus on understanding and responding to car-related commands and queries.\n",
            "If multiple actions are requested, address them in the order mentioned.\n",
            "\n",
            "Current conversation:\n",
            "Human: i feel cold\n",
            "AI: I can adjust the temperature for you. Would you like it to be warmer? \n",
            "\n",
            "Human: on my way to home play some music\n",
            "AI:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Sure! Turning on some rock music for your drive home. Buckle up and enjoy the ride! ð \n",
            "\n",
            "Your command: exit\n",
            "Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Adding Agents\n",
        "\n",
        " The use of LangChain's tool-based architecture allows the assistant to manage complex commands more effectively and provide more accurate and helpful responses compared to the previous implementation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XQVEmRV9lCg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "import requests\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key='KEY')\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"KEY\"\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"],\n",
        "            \"icon\": data[\"weather\"][0][\"icon\"]\n",
        "        }\n",
        "        return f\"Weather in {city}: Temperature: {weather_data['temperature']}Â°C, Description: {weather_data['description']}\"\n",
        "    else:\n",
        "        return f\"Could not find weather information for {city}.\"\n",
        "\n",
        "def play_music(genre=None):\n",
        "    genre = genre or user_preferences[\"music\"]\n",
        "    return f\"Playing {genre} music.\"\n",
        "\n",
        "def navigate(destination):\n",
        "    return f\"Navigating to {destination} using {user_preferences['navigation']}.\"\n",
        "\n",
        "def adjust_climate(setting=None):\n",
        "    setting = setting or user_preferences[\"climate\"]\n",
        "    return f\"Adjusting climate to {setting}.\"\n",
        "\n",
        "# Initialize the search tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Define the tools\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Weather\",\n",
        "        func=get_weather,\n",
        "        description=\"Get the current weather in a given city.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Music\",\n",
        "        func=play_music,\n",
        "        description=\"Play music of a specified genre.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Navigation\",\n",
        "        func=navigate,\n",
        "        description=\"Navigate to a specified destination.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Climate\",\n",
        "        func=adjust_climate,\n",
        "        description=\"Adjust the car's climate settings.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"Search the internet for general information.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are an AI assistant for a smart car. You can help with playing music, navigation, climate control, providing weather information, and searching for general information.\n",
        "Current user preferences:\n",
        "- Music: {music_preference}\n",
        "- Navigation: {navigation_preference}\n",
        "- Climate: {climate_preference}\n",
        "\n",
        "Use the provided tools to assist the user with their requests. If multiple actions are requested, address them in the order mentioned.\n",
        "Always strive to give helpful and accurate responses.\n",
        "\n",
        "{chat_history}\n",
        "Human: {input}\n",
        "AI: Let's approach this step-by-step:\n",
        "\"\"\",\n",
        "    input_variables=[\"chat_history\", \"input\", \"music_preference\", \"navigation_preference\", \"climate_preference\"]\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"system_message\": custom_prompt.format(\n",
        "        music_preference=user_preferences[\"music\"],\n",
        "        navigation_preference=user_preferences[\"navigation\"],\n",
        "        climate_preference=user_preferences[\"climate\"],\n",
        "        chat_history=\"\",\n",
        "        input=\"\"\n",
        "    )}\n",
        ")\n",
        "\n",
        "# Main interaction loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your car's AI assistant! How can I help you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"Your command: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = agent.run(user_input)\n",
        "            print(\"AI:\", response)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            print(\"I apologize for the inconvenience. Is there anything else I can help you with?\")"
      ],
      "metadata": {
        "id": "rhlP0DV2W1Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed0bd5f-23e8-4d3a-9709-aa16b4fe0274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to your car's AI assistant! How can I help you today?\n",
            "Your command: play music while going to work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Navigation\",\n",
            "    \"action_input\": \"Navigate to work\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mNavigating to Navigate to work using shortest route.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Music\",\n",
            "    \"action_input\": \"Play rock music\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPlaying Play rock music music.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Okay, I am now playing rock music on our way to work.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Okay, I am now playing rock music on our way to work.\n",
            "Your command: what route should i take today on my way to home\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Navigation\",\n",
            "    \"action_input\": \"Home\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mNavigating to Home using shortest route.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "\"action\": \"Final Answer\",\n",
            "\"action_input\": \"You are currently being navigated home using the shortest route.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: You are currently being navigated home using the shortest route.\n",
            "Your command: i am feeling too hot\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Climate\",\n",
            "    \"action_input\": \"Decrease cabin temperature\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAdjusting climate to Decrease cabin temperature.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Okay, I have adjusted the temperature to be cooler.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Okay, I have adjusted the temperature to be cooler.\n",
            "Your command: whats the weather like today\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Weather\",\n",
            "    \"action_input\": \"current location\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mCould not find weather information for current location.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"I am sorry, I do not have the weather information for your current location.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I am sorry, I do not have the weather information for your current location.\n",
            "Your command: exit\n",
            "Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Added Vector Databases\n",
        "\n",
        "Enhances the smart car AI assistant by incorporating vector databases and additional functionalities. The integration of vector databases enables efficient and accurate retrieval of information from the car manual and other large documents, improving the overall user experience. Here are the key improvements and additions:\n",
        "\n",
        "Vector Database for Car Manual:\n",
        "\n",
        "\n",
        "\n",
        "- Loading and Caching: The car manual is loaded asynchronously and stored in a vector database (FAISS) for efficient retrieval.\n",
        "Querying the Manual: Users can query the car manual for specific information, and the system retrieves the most relevant sections using vector embeddings.\n",
        "New Tools and Functionalities:\n",
        "\n",
        "- Trip Planning: A new tool for planning trips based on user queries, generating a structured response with destinations, duration, and activities.\n",
        "Expanded Toolset: The assistant now includes tools for playing music, navigation, climate control, weather information, internet searches, and car manual queries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SErrolNnZMZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from threading import Thread\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import requests\n",
        "import json\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key='KEY')\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"KEY\"\n",
        "\n",
        "# Structured output parser for weather\n",
        "weather_schema = ResponseSchema(name=\"weather\", description=\"Weather information including temperature and description\")\n",
        "weather_parser = StructuredOutputParser.from_response_schemas([weather_schema])\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"]\n",
        "        }\n",
        "        return weather_parser.parse(f\"Weather in {city}: Temperature: {weather_data['temperature']}Â°C, Description: {weather_data['description']}\")\n",
        "    else:\n",
        "        return f\"Could not find weather information for {city}.\"\n",
        "\n",
        "def play_music(genre=None):\n",
        "    genre = genre or user_preferences[\"music\"]\n",
        "    return f\"Playing {genre} music.\"\n",
        "\n",
        "def navigate(destination):\n",
        "    return f\"Navigating to {destination} using {user_preferences['navigation']}.\"\n",
        "\n",
        "def adjust_climate(setting=None):\n",
        "    setting = setting or user_preferences[\"climate\"]\n",
        "    return f\"Adjusting climate to {setting}.\"\n",
        "\n",
        "# Trip planning tool\n",
        "class TripPlan(BaseModel):\n",
        "    destinations: List[str] = Field(description=\"List of destinations to visit\")\n",
        "    duration: int = Field(description=\"Trip duration in days\")\n",
        "    activities: List[str] = Field(description=\"List of planned activities\")\n",
        "\n",
        "def plan_trip(query: str) -> str:\n",
        "    trip_chain = LLMChain(llm=llm, prompt=PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\"Plan a trip based on this query: {query}. Provide a list of destinations, trip duration in days, and planned activities. Format your response as a JSON object with keys 'destinations' (list of strings), 'duration' (integer), and 'activities' (list of strings).\"\n",
        "    ))\n",
        "\n",
        "    try:\n",
        "        result = trip_chain.run(query)\n",
        "        trip_data = json.loads(result)\n",
        "        trip_plan = TripPlan(**trip_data)\n",
        "        return f\"Trip Plan: Destinations: {', '.join(trip_plan.destinations)}, Duration: {trip_plan.duration} days, Activities: {', '.join(trip_plan.activities)}\"\n",
        "    except Exception as e:\n",
        "        return f\"I apologize, I couldn't create a proper trip plan. Here's the raw output: {result}\"\n",
        "\n",
        "# Global variable for car manual QA\n",
        "car_manual_qa = None\n",
        "\n",
        "def load_car_manual():\n",
        "    cache_path = \"car_manual_cache.pkl\"\n",
        "    if os.path.exists(cache_path):\n",
        "        print(\"Loading car manual from cache...\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            cache_data = pickle.load(f)\n",
        "\n",
        "        texts = cache_data[\"texts\"]\n",
        "        embeddings = cache_data[\"embeddings\"]\n",
        "\n",
        "        # Recreate the vectorstore\n",
        "        vectorstore = FAISS.from_embeddings(text_embeddings=zip([text.page_content for text in texts], embeddings), embedding=HuggingFaceEmbeddings())\n",
        "\n",
        "        return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
        "    else:\n",
        "        print(\"Error: Car manual cache not found. Please run the caching script first.\")\n",
        "        return None\n",
        "\n",
        "def load_manual_async():\n",
        "    global car_manual_qa\n",
        "    try:\n",
        "        car_manual_qa = load_car_manual()\n",
        "        if car_manual_qa:\n",
        "            print(\"Car manual fully loaded and ready!\")\n",
        "        else:\n",
        "            print(\"Failed to load car manual.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading car manual: {str(e)}\")\n",
        "        car_manual_qa = None\n",
        "\n",
        "# Start loading the car manual in a separate thread\n",
        "Thread(target=load_manual_async, daemon=True).start()\n",
        "\n",
        "def car_manual_query(query):\n",
        "    if car_manual_qa:\n",
        "        try:\n",
        "            return car_manual_qa.run(query)\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred while querying the car manual: {str(e)}\"\n",
        "    else:\n",
        "        return \"The car manual is still loading or encountered an error during loading. Please try again later.\"\n",
        "\n",
        "# Initialize the search tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Define the tools\n",
        "tools = [\n",
        "    Tool(name=\"Weather\", func=get_weather, description=\"Get the current weather in a given city.\"),\n",
        "    Tool(name=\"Music\", func=play_music, description=\"Play music of a specified genre.\"),\n",
        "    Tool(name=\"Navigation\", func=navigate, description=\"Navigate to a specified destination.\"),\n",
        "    Tool(name=\"Climate\", func=adjust_climate, description=\"Adjust the car's climate settings.\"),\n",
        "    Tool(name=\"Search\", func=search.run, description=\"Search the internet for general information.\"),\n",
        "    Tool(name=\"TripPlanner\", func=plan_trip, description=\"Plan a trip with destinations, duration, and activities.\"),\n",
        "    Tool(name=\"CarManual\", func=car_manual_query, description=\"Get information from the car manual.\")\n",
        "]\n",
        "\n",
        "# Initialize the memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are an advanced AI assistant for a smart car. You can help with various tasks including playing music, navigation, climate control, weather information, trip planning, car manual inquiries, and general information searches.\n",
        "Current user preferences:\n",
        "- Music: {music_preference}\n",
        "- Navigation: {navigation_preference}\n",
        "- Climate: {climate_preference}\n",
        "\n",
        "Use the provided tools to assist the user with their requests. If multiple actions are requested, address them in the order mentioned.\n",
        "For complex queries, think step-by-step and explain your reasoning.\n",
        "\n",
        "{chat_history}\n",
        "Human: {input}\n",
        "AI: Let's approach this step-by-step:\n",
        "\"\"\",\n",
        "    input_variables=[\"chat_history\", \"input\", \"music_preference\", \"navigation_preference\", \"climate_preference\"]\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"system_message\": custom_prompt.format(\n",
        "        music_preference=user_preferences[\"music\"],\n",
        "        navigation_preference=user_preferences[\"navigation\"],\n",
        "        climate_preference=user_preferences[\"climate\"],\n",
        "        chat_history=\"\",\n",
        "        input=\"\"\n",
        "    )}\n",
        ")\n",
        "\n",
        "# Main interaction loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your advanced car's AI assistant! How can I help you today?\")\n",
        "    print(\"(The car manual is being loaded in the background and will be available shortly.)\")\n",
        "    while True:\n",
        "        user_input = input(\"Your command: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = agent.run(user_input)\n",
        "            print(\"AI:\", response)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            print(\"I apologize for the inconvenience. Is there anything else I can help you with?\")"
      ],
      "metadata": {
        "id": "kRVmeH26Xbc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc285e8d-b236-4c56-c638-08cdf11408dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading car manual from cache...\n",
            "Welcome to your advanced car's AI assistant! How can I help you today?\n",
            "(The car manual is being loaded in the background and will be available shortly.)\n",
            "Car manual fully loaded and ready!\n",
            "Your command: what is my car name\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"I am sorry, I do not have access to your car's information and therefore do not know your car's name.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I am sorry, I do not have access to your car's information and therefore do not know your car's name.\n",
            "Your command: what is my car name form my carmanual\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"CarManual\",\n",
            "    \"action_input\": \"Find car name\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe provided text does not contain the car's name. \n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "\"action\": \"Final Answer\",\n",
            "\"action_input\": \"Unfortunately, I still don't have access to that information. The car manual did not provide your car's name.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Unfortunately, I still don't have access to that information. The car manual did not provide your car's name.\n",
            "Your command: what are the featured in the dashboard in the carmanual\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"CarManual\",\n",
            "    \"action_input\": \"What are the featured items in the dashboard?\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe dashboard, also known as the instrument cluster, features:\n",
            "\n",
            "* **Tachometer**\n",
            "* **Speedometer** (available as both analog and digital displays)\n",
            "* **Fuel Gauge**\n",
            "* **Engine Coolant Temperature Gauge**\n",
            "* **Warning and Indicator Lights**\n",
            "* **Cluster Display** (including Trip Computer)\n",
            "* **Trip Mode/Reset Button**\n",
            "* **Gear Position Pop-Up** (optional)\n",
            "* **Service Interval Indicator**\n",
            "* **Elapsed Time** (if equipped) \n",
            "\n",
            "The exact features and layout may vary depending on the specific vehicle model and trim level (\"Type A\", \"Type B\", or \"Type C\"). \n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The dashboard, also known as the instrument cluster, features a tachometer, speedometer, fuel gauge, engine coolant temperature gauge, warning and indicator lights, a cluster display which includes the trip computer, a trip mode/reset button, a gear position pop-up, a service interval indicator, and elapsed time.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: The dashboard, also known as the instrument cluster, features a tachometer, speedometer, fuel gauge, engine coolant temperature gauge, warning and indicator lights, a cluster display which includes the trip computer, a trip mode/reset button, a gear position pop-up, a service interval indicator, and elapsed time.\n",
            "Your command: exit\n",
            "Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Caching The Pdf\n",
        "\n",
        "This script caches the car manual to enhance retrieval efficiency by processing and embedding the manual's content."
      ],
      "metadata": {
        "id": "ZgQ_6D_UNjY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def cache_car_manual(pdf_path, cache_path=\"car_manual_cache.pkl\"):\n",
        "    print(\"Processing car manual. This may take a while...\")\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    embedded_texts = [embeddings.embed_query(text.page_content) for text in texts]\n",
        "\n",
        "    cache_data = {\n",
        "        \"texts\": texts,\n",
        "        \"embeddings\": embedded_texts\n",
        "    }\n",
        "\n",
        "    print(\"Saving car manual to cache...\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "\n",
        "    print(f\"Car manual cached successfully to {cache_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"grand-i10-nios.pdf\"  # Update this path to your car manual PDF\n",
        "    cache_path = \"car_manual_cache.pkl\"\n",
        "\n",
        "    if os.path.exists(pdf_path):\n",
        "        cache_car_manual(pdf_path, cache_path)\n",
        "    else:\n",
        "        print(f\"Error: Car manual PDF not found at {pdf_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCWzqlK-Nh0e",
        "outputId": "a7f7efee-c69d-4dd9-fff3-73c87a10ae8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing car manual. This may take a while...\n",
            "Saving car manual to cache...\n",
            "Car manual cached successfully to car_manual_cache.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the ngrok auth token which is needed to run our Streamlit app"
      ],
      "metadata": {
        "id": "PiLn0cJjawaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken YOUR-AUTH-TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSiMYGrI1pdo",
        "outputId": "ac6ae686-5e99-45eb-9118-66b380371e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Exposing The Endpoint\n",
        "\n",
        "This code sets up a FastAPI server to provide API access to an advanced AI assistant for a smart car. Here's how it facilitates API-based interaction:\n",
        "\n",
        "### FastAPI Server Setup:\n",
        "\n",
        "- FastAPI Framework: The FastAPI framework is used to create a web server that handles HTTP requests and serves responses.\n",
        "CORS Middleware: Configured to allow cross-origin requests from specific domains (e.g., localhost and deployed front-end applications).\n",
        "API Endpoint:\n",
        "\n",
        "- /chat Endpoint: A POST request to /chat accepts user messages. This endpoint uses the AI agent to process the message and generate a response.\n",
        "Pydantic Model: The UserMessage model ensures that the incoming requests have a message field, enforcing data validation.\n",
        "\n",
        "### Ngrok Integration:\n",
        "\n",
        "- Public URL Exposure: Ngrok is used to expose the local FastAPI server to the public internet, allowing external applications to interact with the API. The public URL provided by ngrok can be used for remote access and testing."
      ],
      "metadata": {
        "id": "jc3_I_EGb0As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from threading import Thread\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "from typing import List\n",
        "import requests\n",
        "import json\n",
        "import uvicorn\n",
        "from langchain.vectorstores import FAISS\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'Key'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key='Key')\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"Key\"\n",
        "\n",
        "# Structured output parser for weather\n",
        "weather_schema = ResponseSchema(name=\"weather\", description=\"Weather information including temperature and description\")\n",
        "weather_parser = StructuredOutputParser.from_response_schemas([weather_schema])\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"]\n",
        "        }\n",
        "        return weather_parser.parse(f\"Weather in {city}: Temperature: {weather_data['temperature']}Â°C, Description: {weather_data['description']}\")\n",
        "    else:\n",
        "        return f\"Could not find weather information for {city}.\"\n",
        "\n",
        "def play_music(genre=None):\n",
        "    genre = genre or user_preferences[\"music\"]\n",
        "    return f\"Playing {genre} music.\"\n",
        "\n",
        "def navigate(destination):\n",
        "    return f\"Navigating to {destination} using {user_preferences['navigation']}.\"\n",
        "\n",
        "def adjust_climate(setting=None):\n",
        "    setting = setting or user_preferences[\"climate\"]\n",
        "    return f\"Adjusting climate to {setting}.\"\n",
        "\n",
        "# Trip planning tool\n",
        "class TripPlan(BaseModel):\n",
        "    destinations: List[str] = Field(description=\"List of destinations to visit\")\n",
        "    duration: int = Field(description=\"Trip duration in days\")\n",
        "    activities: List[str] = Field(description=\"List of planned activities\")\n",
        "\n",
        "def plan_trip(query: str) -> str:\n",
        "    trip_chain = LLMChain(llm=llm, prompt=PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\"Plan a trip based on this query: {query}. Provide a list of destinations, trip duration in days, and planned activities. Format your response as a JSON object with keys 'destinations' (list of strings), 'duration' (integer), and 'activities' (list of strings).\"\n",
        "    ))\n",
        "\n",
        "    try:\n",
        "        result = trip_chain.run(query)\n",
        "        trip_data = json.loads(result)\n",
        "        trip_plan = TripPlan(**trip_data)\n",
        "        return f\"Trip Plan: Destinations: {', '.join(trip_plan.destinations)}, Duration: {trip_plan.duration} days, Activities: {', '.join(trip_plan.activities)}\"\n",
        "    except Exception as e:\n",
        "        return f\"I apologize, I couldn't create a proper trip plan. Here's the raw output: {result}\"\n",
        "\n",
        "# Global variable for car manual QA\n",
        "car_manual_qa = None\n",
        "\n",
        "def load_car_manual():\n",
        "    cache_path = \"car_manual_cache.pkl\"\n",
        "    if os.path.exists(cache_path):\n",
        "        print(\"Loading car manual from cache...\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            cache_data = pickle.load(f)\n",
        "\n",
        "        texts = cache_data[\"texts\"]\n",
        "        embeddings = cache_data[\"embeddings\"]\n",
        "\n",
        "        # Recreate the vectorstore\n",
        "        vectorstore = FAISS.from_embeddings(text_embeddings=zip([text.page_content for text in texts], embeddings), embedding=HuggingFaceEmbeddings())\n",
        "\n",
        "        return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
        "    else:\n",
        "        print(\"Error: Car manual cache not found. Please run the caching script first.\")\n",
        "        return None\n",
        "\n",
        "def load_manual_async():\n",
        "    global car_manual_qa\n",
        "    try:\n",
        "        car_manual_qa = load_car_manual()\n",
        "        if car_manual_qa:\n",
        "            print(\"Car manual fully loaded and ready!\")\n",
        "        else:\n",
        "            print(\"Failed to load car manual.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading car manual: {str(e)}\")\n",
        "        car_manual_qa = None\n",
        "\n",
        "# Start loading the car manual in a separate thread\n",
        "Thread(target=load_manual_async, daemon=True).start()\n",
        "\n",
        "def car_manual_query(query):\n",
        "    if car_manual_qa:\n",
        "        try:\n",
        "            return car_manual_qa.run(query)\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred while querying the car manual: {str(e)}\"\n",
        "    else:\n",
        "        return \"The car manual is still loading or encountered an error during loading. Please try again later.\"\n",
        "\n",
        "# Initialize the search tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Define the tools\n",
        "tools = [\n",
        "    Tool(name=\"Weather\", func=get_weather, description=\"Get the current weather in a given city.\"),\n",
        "    Tool(name=\"Music\", func=play_music, description=\"Play music of a specified genre.\"),\n",
        "    Tool(name=\"Navigation\", func=navigate, description=\"Navigate to a specified destination.\"),\n",
        "    Tool(name=\"Climate\", func=adjust_climate, description=\"Adjust the car's climate settings.\"),\n",
        "    Tool(name=\"Search\", func=search.run, description=\"Search the internet for general information.\"),\n",
        "    Tool(name=\"TripPlanner\", func=plan_trip, description=\"Plan a trip with destinations, duration, and activities.\"),\n",
        "    Tool(name=\"CarManual\", func=car_manual_query, description=\"Get information from the car manual.\")\n",
        "]\n",
        "\n",
        "# Initialize the memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are an advanced AI assistant for a smart car. You can help with various tasks including playing music, navigation, climate control, weather information, trip planning, car manual inquiries, and general information searches.\n",
        "Current user preferences:\n",
        "- Music: {music_preference}\n",
        "- Navigation: {navigation_preference}\n",
        "- Climate: {climate_preference}\n",
        "\n",
        "Use the provided tools to assist the user with their requests. If multiple actions are requested, address them in the order mentioned.\n",
        "For complex queries, think step-by-step and explain your reasoning.\n",
        "\n",
        "{chat_history}\n",
        "Human: {input}\n",
        "AI: Let's approach this step-by-step:\n",
        "\"\"\",\n",
        "    input_variables=[\"chat_history\", \"input\", \"music_preference\", \"navigation_preference\", \"climate_preference\"]\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"system_message\": custom_prompt.format(\n",
        "        music_preference=user_preferences[\"music\"],\n",
        "        navigation_preference=user_preferences[\"navigation\"],\n",
        "        climate_preference=user_preferences[\"climate\"],\n",
        "        chat_history=\"\",\n",
        "        input=\"\"\n",
        "    )}\n",
        ")\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "origins = [\n",
        "    \"http://localhost:3000\",\n",
        "    \"https://car-dashboard-ivory.vercel.app\"\n",
        "]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Pydantic model for the request\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Endpoint to handle user messages\n",
        "@app.post(\"/chat\")\n",
        "async def chat(user_message: UserMessage):\n",
        "    try:\n",
        "        response = agent.run(user_message.message)\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Main function to run the FastAPI server with ngrok\n",
        "def run():\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    config = uvicorn.Config(app, port=8000, loop=\"asyncio\")\n",
        "    server = uvicorn.Server(config)\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        executor.submit(server.run)\n",
        "\n",
        "    # Keep the main thread alive\n",
        "    while True:\n",
        "        try:\n",
        "            asyncio.sleep(1)\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting the AI assistant server...\")\n",
        "    print(\"The car manual is being loaded in the background and will be available shortly.\")\n",
        "\n",
        "    # Start loading the car manual in a separate thread\n",
        "    Thread(target=load_manual_async, daemon=True).start()\n",
        "\n",
        "    # Run the FastAPI server with ngrok\n",
        "    run()"
      ],
      "metadata": {
        "id": "ADN9-wMO0fde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging\n",
        "\n",
        "The script provided is designed to interact with a user via a console-based AI assistant, and it includes functionality for logging responses and user interactions. Here's how it manages and saves data for future use:\n",
        "\n",
        "### Interaction Logging:\n",
        "\n",
        "- Conversation Data: Each interaction with the AI assistant is logged, including user inputs, AI responses, tool usage, timestamps, processing times, and any errors.\n",
        "- Session Management: A unique session ID is generated for each session, and interactions are grouped under this session ID.\n",
        "\n",
        "### CSV Conversion for Fine-Tuning:\n",
        "\n",
        "- Data Extraction: After logging interactions in JSON format, the responses and related data can be extracted and converted to a CSV file.\n",
        "- CSV Benefits: CSV files are widely used for data manipulation and analysis. They are useful for preparing datasets for model fine-tuning and other data-driven tasks.\n",
        "Future Fine-Tuning: The saved CSV data can be used to retrain or fine-tune the AI model, improving its performance based on real-world interactions and feedback."
      ],
      "metadata": {
        "id": "sbPCa-PIZS2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "from threading import Thread\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import requests\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'KEY'\n",
        "\n",
        "# Initialize the language model\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key='KEY')\n",
        "\n",
        "# User preferences\n",
        "user_preferences = {\n",
        "    \"music\": \"rock\",\n",
        "    \"navigation\": \"shortest route\",\n",
        "    \"climate\": \"cool\",\n",
        "}\n",
        "\n",
        "# API key for weather service\n",
        "WEATHER_API_KEY = \"KEY\"\n",
        "\n",
        "# Structured output parser for weather\n",
        "weather_schema = ResponseSchema(name=\"weather\", description=\"Weather information including temperature and description\")\n",
        "weather_parser = StructuredOutputParser.from_response_schemas([weather_schema])\n",
        "\n",
        "def get_weather(city='San Francisco'):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    if response.status_code == 200:\n",
        "        weather_data = {\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"description\": data[\"weather\"][0][\"description\"]\n",
        "        }\n",
        "        return weather_parser.parse(f\"Weather in {city}: Temperature: {weather_data['temperature']}Â°C, Description: {weather_data['description']}\")\n",
        "    else:\n",
        "        return f\"Could not find weather information for {city}.\"\n",
        "\n",
        "def play_music(genre=None):\n",
        "    genre = genre or user_preferences[\"music\"]\n",
        "    return f\"Playing {genre} music.\"\n",
        "\n",
        "def navigate(destination):\n",
        "    return f\"Navigating to {destination} using {user_preferences['navigation']}.\"\n",
        "\n",
        "def adjust_climate(setting=None):\n",
        "    setting = setting or user_preferences[\"climate\"]\n",
        "    return f\"Adjusting climate to {setting}.\"\n",
        "\n",
        "# Trip planning tool\n",
        "class TripPlan(BaseModel):\n",
        "    destinations: List[str] = Field(description=\"List of destinations to visit\")\n",
        "    duration: int = Field(description=\"Trip duration in days\")\n",
        "    activities: List[str] = Field(description=\"List of planned activities\")\n",
        "\n",
        "def plan_trip(query: str) -> str:\n",
        "    trip_chain = LLMChain(llm=llm, prompt=PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\"Plan a trip based on this query: {query}. Provide a list of destinations, trip duration in days, and planned activities. Format your response as a JSON object with keys 'destinations' (list of strings), 'duration' (integer), and 'activities' (list of strings).\"\n",
        "    ))\n",
        "\n",
        "    try:\n",
        "        result = trip_chain.run(query)\n",
        "        trip_data = json.loads(result)\n",
        "        trip_plan = TripPlan(**trip_data)\n",
        "        return f\"Trip Plan: Destinations: {', '.join(trip_plan.destinations)}, Duration: {trip_plan.duration} days, Activities: {', '.join(trip_plan.activities)}\"\n",
        "    except Exception as e:\n",
        "        return f\"I apologize, I couldn't create a proper trip plan. Here's the raw output: {result}\"\n",
        "\n",
        "# Global variable for car manual QA\n",
        "car_manual_qa = None\n",
        "\n",
        "def load_car_manual():\n",
        "    cache_path = \"car_manual_cache.pkl\"\n",
        "    if os.path.exists(cache_path):\n",
        "        print(\"Loading car manual from cache...\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            cache_data = pickle.load(f)\n",
        "\n",
        "        texts = cache_data[\"texts\"]\n",
        "        embeddings = cache_data[\"embeddings\"]\n",
        "\n",
        "        # Recreate the vectorstore\n",
        "        vectorstore = FAISS.from_embeddings(text_embeddings=zip([text.page_content for text in texts], embeddings), embedding=HuggingFaceEmbeddings())\n",
        "\n",
        "        return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
        "    else:\n",
        "        print(\"Error: Car manual cache not found. Please run the caching script first.\")\n",
        "        return None\n",
        "\n",
        "def load_manual_async():\n",
        "    global car_manual_qa\n",
        "    try:\n",
        "        car_manual_qa = load_car_manual()\n",
        "        if car_manual_qa:\n",
        "            print(\"Car manual fully loaded and ready!\")\n",
        "        else:\n",
        "            print(\"Failed to load car manual.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading car manual: {str(e)}\")\n",
        "        car_manual_qa = None\n",
        "\n",
        "# Start loading the car manual in a separate thread\n",
        "Thread(target=load_manual_async, daemon=True).start()\n",
        "\n",
        "def car_manual_query(query):\n",
        "    if car_manual_qa:\n",
        "        try:\n",
        "            return car_manual_qa.run(query)\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred while querying the car manual: {str(e)}\"\n",
        "    else:\n",
        "        return \"The car manual is still loading or encountered an error during loading. Please try again later.\"\n",
        "\n",
        "# Initialize the search tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Define the tools\n",
        "tools = [\n",
        "    Tool(name=\"Weather\", func=get_weather, description=\"Get the current weather in a given city.\"),\n",
        "    Tool(name=\"Music\", func=play_music, description=\"Play music of a specified genre.\"),\n",
        "    Tool(name=\"Navigation\", func=navigate, description=\"Navigate to a specified destination.\"),\n",
        "    Tool(name=\"Climate\", func=adjust_climate, description=\"Adjust the car's climate settings.\"),\n",
        "    Tool(name=\"Search\", func=search.run, description=\"Search the internet for general information.\"),\n",
        "    Tool(name=\"TripPlanner\", func=plan_trip, description=\"Plan a trip with destinations, duration, and activities.\"),\n",
        "    Tool(name=\"CarManual\", func=car_manual_query, description=\"Get information from the car manual.\")\n",
        "]\n",
        "\n",
        "# Initialize the memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"\"\"You are an advanced AI assistant for a smart car. You can help with various tasks including playing music, navigation, climate control, weather information, trip planning, car manual inquiries, and general information searches.\n",
        "Current user preferences:\n",
        "- Music: {music_preference}\n",
        "- Navigation: {navigation_preference}\n",
        "- Climate: {climate_preference}\n",
        "\n",
        "Use the provided tools to assist the user with their requests. If multiple actions are requested, address them in the order mentioned.\n",
        "For complex queries, think step-by-step and explain your reasoning.\n",
        "\n",
        "{chat_history}\n",
        "Human: {input}\n",
        "AI: Let's approach this step-by-step:\n",
        "\"\"\",\n",
        "    input_variables=[\"chat_history\", \"input\", \"music_preference\", \"navigation_preference\", \"climate_preference\"]\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_kwargs={\"system_message\": custom_prompt.format(\n",
        "        music_preference=user_preferences[\"music\"],\n",
        "        navigation_preference=user_preferences[\"navigation\"],\n",
        "        climate_preference=user_preferences[\"climate\"],\n",
        "        chat_history=\"\",\n",
        "        input=\"\"\n",
        "    )}\n",
        ")\n",
        "\n",
        "# Initialize or load existing conversation data\n",
        "try:\n",
        "    with open('conversation_data.json', 'r') as f:\n",
        "        conversation_data = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    conversation_data = []\n",
        "\n",
        "# Function to get current user context\n",
        "def get_user_context():\n",
        "    return {\n",
        "        \"music_preference\": user_preferences[\"music\"],\n",
        "        \"navigation_preference\": user_preferences[\"navigation\"],\n",
        "        \"climate_preference\": user_preferences[\"climate\"],\n",
        "        # Add any other relevant context here\n",
        "    }\n",
        "\n",
        "# Function to log tool usage\n",
        "def log_tool_usage(tool_name, input_data, output_data):\n",
        "    return {\n",
        "        \"tool_name\": tool_name,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "# Wrapper function for tools to log their usage\n",
        "def tool_wrapper(tool):\n",
        "    original_func = tool.func\n",
        "    def wrapped_func(*args, **kwargs):\n",
        "        result = original_func(*args, **kwargs)\n",
        "        log_entry = log_tool_usage(tool.name, args[0] if args else kwargs, result)\n",
        "        conversation_data[-1][\"tool_usage\"].append(log_entry)\n",
        "        return result\n",
        "    tool.func = wrapped_func\n",
        "    return tool\n",
        "\n",
        "# Wrap all tools with the logging function\n",
        "tools = [tool_wrapper(tool) for tool in tools]\n",
        "\n",
        "# Main interaction loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your advanced car's AI assistant! How can I help you today?\")\n",
        "    print(\"(The car manual is being loaded in the background and will be available shortly.)\")\n",
        "\n",
        "    session_start_time = datetime.now().isoformat()\n",
        "    session_id = f\"session_{session_start_time}\"\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Your command: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Start timing the response\n",
        "            start_time = datetime.now()\n",
        "\n",
        "            # Create a new entry for this interaction\n",
        "            current_interaction = {\n",
        "                \"session_id\": session_id,\n",
        "                \"timestamp\": start_time.isoformat(),\n",
        "                \"user_input\": user_input,\n",
        "                \"user_context\": get_user_context(),\n",
        "                \"tool_usage\": [],\n",
        "                \"processing_time\": None,\n",
        "                \"ai_response\": None,\n",
        "                \"error\": None\n",
        "            }\n",
        "\n",
        "            # Add the new interaction to the conversation data\n",
        "            conversation_data.append(current_interaction)\n",
        "\n",
        "            response = agent.run(user_input)\n",
        "            print(\"AI:\", response)\n",
        "\n",
        "            # Calculate processing time\n",
        "            end_time = datetime.now()\n",
        "            processing_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "            # Update the current interaction with the response and processing time\n",
        "            current_interaction[\"ai_response\"] = response\n",
        "            current_interaction[\"processing_time\"] = processing_time\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            print(\"I apologize for the inconvenience. Is there anything else I can help you with?\")\n",
        "\n",
        "            # Update the current interaction with error information\n",
        "            current_interaction[\"error\"] = str(e)\n",
        "\n",
        "        # Save to file after each interaction\n",
        "        with open('conversation_data.json', 'w') as f:\n",
        "            json.dump(conversation_data, f, indent=2)\n",
        "\n",
        "    # Log session end time\n",
        "    session_end_time = datetime.now().isoformat()\n",
        "    conversation_data.append({\n",
        "        \"session_id\": session_id,\n",
        "        \"session_end_time\": session_end_time\n",
        "    })\n",
        "\n",
        "    # Final save to file\n",
        "    with open('conversation_data.json', 'w') as f:\n",
        "        json.dump(conversation_data, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9UgHOf0ZWXU",
        "outputId": "ca10d4fd-548c-4589-96d0-c4e792714603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Car manual cache not found. Please run the caching script first.\n",
            "Failed to load car manual.\n",
            "Welcome to your advanced car's AI assistant! How can I help you today?\n",
            "(The car manual is being loaded in the background and will be available shortly.)\n",
            "Your command: hello\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "\"action\": \"Final Answer\",\n",
            "\"action_input\": \"Hello! How can I assist you today?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Hello! How can I assist you today?\n",
            "Your command: what is the weather today\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Weather\",\n",
            "    \"action_input\": \"current location\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mCould not find weather information for current location.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"I'm sorry, I do not have the weather information for your current location.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I'm sorry, I do not have the weather information for your current location.\n",
            "Your command: i want to hear ro some music\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Music\",\n",
            "    \"action_input\": \"rock\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPlaying rock music.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"I am ready for your next request.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I am ready for your next request.\n",
            "Your command: i want to go home and search about travel places\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Navigation\",\n",
            "    \"action_input\": \"Home\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mNavigating to Home using shortest route.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Search\",\n",
            "    \"action_input\": \"travel places\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mParis. #1 in World's Best Places to Visit for 2023-2024. France's magnetic City of Light is a perennial tourist destination, drawing visitors with its iconic attractions, like the Eiffel Tower and ... Glacier National Park. #1 in Best Places to Visit in the USA for 2023-2024. Glaciers are the main draw of this Montana national park, but its more than 700 lakes, two mountain ranges and multiple ... What are the best places to visit in the world? Time magazine just revealed its annual World's Greatest Places List, which highlights 100 amazing places around the globeâincluding hotels ... Our guidebooks & travel books. Whether you're interested in traveling to a new city, going on a cruise, or cooking a new dish â we're committed to inspiring you to experience travel in a whole new way. Lonely Planet's collection of 825+ travel and guidebooks is sure to inspire the traveler within. View All Books. Find out which places made the ultimate ranking of travel destinations based on their beauty, culture, and attractions. From Dubai to Bora Bora, Maui to London, see the most incredible travel destinations on Earth.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"According to a search, some of the best places to travel include Paris, Glacier National Park, and many others. \"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: According to a search, some of the best places to travel include Paris, Glacier National Park, and many others. \n"
          ]
        }
      ]
    }
  ]
}